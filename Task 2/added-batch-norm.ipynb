{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10698484,"sourceType":"datasetVersion","datasetId":6629771}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision scikit-learn Pillow\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport os\nimport string\nimport numpy as np\nfrom tqdm import tqdm\n\n# Dataset Class for Task 2\nclass CaptchaSequenceDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        \n        # Define vocabulary (case-sensitive letters)\n        self.chars = string.ascii_letters  # 52 characters\n        self.char_to_idx = {char: idx+1 for idx, char in enumerate(self.chars)}  # blank=0\n        self.idx_to_char = {idx+1: char for idx, char in enumerate(self.chars)}\n        self.num_classes = len(self.chars) + 1  # 53\n        \n        # Load images and labels\n        for img_name in os.listdir(root_dir):\n            if img_name.endswith('.png'):\n                parts = img_name.split('_')\n                if len(parts) >=3 and parts[0] == 'captcha':\n                    label = parts[1]\n                    # Validate characters\n                    if all(c in self.char_to_idx for c in label):\n                        self.image_paths.append(os.path.join(root_dir, img_name))\n                        self.labels.append(label)\n                    else:\n                        print(f\"Skipped {img_name} due to invalid characters\")\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Convert label to indices\n        target = [self.char_to_idx[c] for c in label]\n        target_length = torch.tensor(len(target), dtype=torch.long)\n        target = torch.tensor(target, dtype=torch.long)\n        \n        return image, target, target_length\n\n# Collate function for DataLoader\ndef collate_fn(batch):\n    images, targets, target_lengths = [], [], []\n    for item in batch:\n        images.append(item[0])\n        targets.append(item[1])\n        target_lengths.append(item[2])\n    images = torch.stack(images)\n    targets = torch.cat(targets)\n    target_lengths = torch.stack(target_lengths)\n    # Assume fixed sequence length (determined by model)\n    input_lengths = torch.full((len(batch),), 64, dtype=torch.long)  # Update based on model\n    return images, targets, input_lengths, target_lengths\n\n# Model Architecture (CRNN with CTC)\nclass CRNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CRNN, self).__init__()\n        self.num_classes = num_classes\n        # CNN layers\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d((2,2)),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d((2,2)),\n            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2,1)),\n            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2,1)),\n            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((4,1))\n        )\n        # RNN layers\n        self.rnn = nn.LSTM(512, 256, bidirectional=True, num_layers=2, dropout=0.3, batch_first=True)\n        self.fc = nn.Linear(512, num_classes)\n        # Vocabulary mapping\n        self.idx_to_char = {idx+1: char for idx, char in enumerate(string.ascii_letters)}\n        self.idx_to_char[0] = '-'  # Blank\n\n    def forward(self, x):\n        x = self.cnn(x)  # (batch, channels, height, width)\n        x = x.squeeze(2)  # Remove height dim\n        x = x.permute(0, 2, 1)  # (batch, seq_len, features)\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        x = nn.functional.log_softmax(x, dim=2)\n        return x\n\n# Transforms\ntrain_transform = transforms.Compose([\n    transforms.Resize((64, 256)),\n    transforms.RandomRotation(5),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_test_transform = transforms.Compose([\n    transforms.Resize((64, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T10:31:49.730040Z","iopub.execute_input":"2025-02-09T10:31:49.730340Z","iopub.status.idle":"2025-02-09T10:32:00.178549Z","shell.execute_reply.started":"2025-02-09T10:31:49.730295Z","shell.execute_reply":"2025-02-09T10:32:00.177902Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Initialize Dataset and DataLoaders\nfull_dataset = CaptchaSequenceDataset(root_dir='/kaggle/input/hard-captcha-data-set-50k/hard_captcha_dataset', transform=None)\ntrain_val_idx, test_idx = train_test_split(\n    range(len(full_dataset)), test_size=0.2, random_state=42, \n   \n)\ntrain_idx, val_idx = train_test_split(\n    train_val_idx, test_size=0.25, random_state=42,  # 25% of train+val for validation\n    \n)\n\nclass TransformedSubset(Dataset):\n    def __init__(self, subset, transform=None):\n        self.subset = subset\n        self.transform = transform\n    def __len__(self):\n        return len(self.subset)\n    def __getitem__(self, idx):\n        x, y, y_len = self.subset[idx]\n        if self.transform:\n            x = self.transform(x)\n        return x, y, y_len\n\ntrain_subset = TransformedSubset(Subset(full_dataset, train_idx), train_transform)\nval_subset = TransformedSubset(Subset(full_dataset, val_idx), val_test_transform)\ntest_subset = TransformedSubset(Subset(full_dataset, test_idx), val_test_transform)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n\n# Training Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CRNN(num_classes=53).to(device)\ncriterion = nn.CTCLoss(blank=0)\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T10:44:26.553987Z","iopub.execute_input":"2025-02-09T10:44:26.554376Z","iopub.status.idle":"2025-02-09T10:44:27.494929Z","shell.execute_reply.started":"2025-02-09T10:44:26.554345Z","shell.execute_reply":"2025-02-09T10:44:27.494269Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Decoding function","metadata":{}},{"cell_type":"code","source":"def decode_predictions(outputs, model):\n    _, max_indices = torch.max(outputs, 2)\n    batch_size = outputs.size(0)\n    decoded = []\n    for i in range(batch_size):\n        indices = max_indices[i].cpu().numpy()\n        chars = []\n        previous = None\n        for idx in indices:\n            if idx != 0:\n                if idx != previous:\n                    chars.append(model.idx_to_char.get(idx, ''))\n            previous = idx\n        decoded.append(''.join(chars))\n    return decoded\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T10:44:30.672758Z","iopub.execute_input":"2025-02-09T10:44:30.673042Z","iopub.status.idle":"2025-02-09T10:44:30.678125Z","shell.execute_reply.started":"2025-02-09T10:44:30.673021Z","shell.execute_reply":"2025-02-09T10:44:30.677055Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"num_epochs = 30\nbest_val_word_acc = 0.0\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    total_train_chars = 0\n    correct_train_chars = 0\n    correct_train_words = 0\n    \n    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n        images, targets, input_lengths, target_lengths = batch\n        images, targets = images.to(device), targets.to(device)\n        input_lengths, target_lengths = input_lengths.to(device), target_lengths.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        log_probs = outputs.permute(1, 0, 2)  # CTC requires (seq_len, batch, num_classes)\n        loss = criterion(log_probs, targets, input_lengths, target_lengths)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        # Decode and calculate accuracy\n        decoded = decode_predictions(outputs, model)\n        for i in range(len(decoded)):\n            true_label = full_dataset.labels[train_idx[batch[0].tolist().index(i)]]\n            pred_label = decoded[i]\n            correct_train_chars += sum(c1 == c2 for c1, c2 in zip(true_label, pred_label))\n            total_train_chars += len(true_label)\n            if pred_label == true_label:\n                correct_train_words += 1\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct_val_chars = 0\n    total_val_chars = 0\n    correct_val_words = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation'):\n            images, targets, input_lengths, target_lengths = batch\n            images, targets = images.to(device), targets.to(device)\n            input_lengths, target_lengths = input_lengths.to(device), target_lengths.to(device)\n            \n            outputs = model(images)\n            log_probs = outputs.permute(1, 0, 2)\n            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n            val_loss += loss.item() * images.size(0)\n            \n            decoded = decode_predictions(outputs, model)\n            for i in range(len(decoded)):\n                true_label = full_dataset.labels[val_idx[batch[0].tolist().index(i)]]\n                pred_label = decoded[i]\n                correct_val_chars += sum(c1 == c2 for c1, c2 in zip(true_label, pred_label))\n                total_val_chars += len(true_label)\n                if pred_label == true_label:\n                    correct_val_words += 1\n    \n    # Calculate metrics\n    train_loss = train_loss / len(train_loader.dataset)\n    train_char_acc = correct_train_chars / total_train_chars if total_train_chars >0 else 0\n    train_word_acc = correct_train_words / len(train_loader.dataset)\n    \n    val_loss = val_loss / len(val_loader.dataset)\n    val_char_acc = correct_val_chars / total_val_chars if total_val_chars >0 else 0\n    val_word_acc = correct_val_words / len(val_loader.dataset)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}:')\n    print(f'Train Loss: {train_loss:.4f} | Char Acc: {train_char_acc:.4f} | Word Acc: {train_word_acc:.4f}')\n    print(f'Val Loss: {val_loss:.4f} | Char Acc: {val_char_acc:.4f} | Word Acc: {val_word_acc:.4f}')\n    \n    scheduler.step(val_loss)\n    \n    if val_word_acc > best_val_word_acc:\n        best_val_word_acc = val_word_acc\n        torch.save(model.state_dict(), 'best_model_task2.pth')\n        print('Saved best model!')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T10:44:33.234709Z","iopub.execute_input":"2025-02-09T10:44:33.235021Z","iopub.status.idle":"2025-02-09T10:44:36.563688Z","shell.execute_reply.started":"2025-02-09T10:44:33.234995Z","shell.execute_reply":"2025-02-09T10:44:36.562506Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/30 - Training:   0%|          | 0/469 [00:03<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-fb83ef1615e9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcorrect_train_chars\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 0 is not in list"],"ename":"ValueError","evalue":"0 is not in list","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Testing\nmodel.load_state_dict(torch.load('best_model_task2.pth'))\nmodel.eval()\ntest_correct_chars = 0\ntest_total_chars = 0\ntest_correct_words = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing'):\n        images, targets, input_lengths, target_lengths = batch\n        images = images.to(device)\n        outputs = model(images)\n        decoded = decode_predictions(outputs, model)\n        for i in range(len(decoded)):\n            true_label = full_dataset.labels[test_idx[batch[0].tolist().index(i)]]\n            pred_label = decoded[i]\n            test_correct_chars += sum(c1 == c2 for c1, c2 in zip(true_label, pred_label))\n            test_total_chars += len(true_label)\n            if pred_label == true_label:\n                test_correct_words += 1\n\ntest_char_acc = test_correct_chars / test_total_chars if test_total_chars >0 else 0\ntest_word_acc = test_correct_words / len(test_loader.dataset)\nprint(f'Test Results: Char Acc: {test_char_acc:.4f} | Word Acc: {test_word_acc:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}